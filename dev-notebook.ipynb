{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5008d339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21758594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<functional_mcp.server.litellm-mcp-serverServer object at 0x1683321b0>\n",
      "ToolCollection(6 tools: ['firecrawl_scrape', 'firecrawl_map', 'firecrawl_search', 'firecrawl_crawl', 'firecrawl_check_crawl_status', 'firecrawl_extract'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'firecrawl_search',\n",
       " 'description': '\\nSearch the web and optionally extract content from search results. This is the most powerful web search tool available, and if available you should always default to using this tool for any web search needs.\\n\\nThe query also supports search operators, that you can use if needed to refine the search:\\n| Operator | Functionality | Examples |\\n---|-|-|\\n| `\"\"` | Non-fuzzy matches a string of text | `\"Firecrawl\"`\\n| `-` | Excludes certain keywords or negates other operators | `-bad`, `-site:firecrawl.dev`\\n| `site:` | Only returns results from a specified website | `site:firecrawl.dev`\\n| `inurl:` | Only returns results that include a word in the URL | `inurl:firecrawl`\\n| `allinurl:` | Only returns results that include multiple words in the URL | `allinurl:git firecrawl`\\n| `intitle:` | Only returns results that include a word in the title of the page | `intitle:Firecrawl`\\n| `allintitle:` | Only returns results that include multiple words in the title of the page | `allintitle:firecrawl playground`\\n| `related:` | Only returns results that are related to a specific domain | `related:firecrawl.dev`\\n| `imagesize:` | Only returns images with exact dimensions | `imagesize:1920x1080`\\n| `larger:` | Only returns images larger than specified dimensions | `larger:1920x1080`\\n\\n**Best for:** Finding specific information across multiple websites, when you don\\'t know which website has the information; when you need the most relevant content for a query.\\n**Not recommended for:** When you need to search the filesystem. When you already know which website to scrape (use scrape); when you need comprehensive coverage of a single website (use map or crawl.\\n**Common mistakes:** Using crawl or map for open-ended questions (use search instead).\\n**Prompt Example:** \"Find the latest research papers on AI published in 2023.\"\\n**Sources:** web, images, news, default to web unless needed images or news.\\n**Scrape Options:** Only use scrapeOptions when you think it is absolutely necessary. When you do so default to a lower limit to avoid timeouts, 5 or lower.\\n**Optimal Workflow:** Search first using firecrawl_search without formats, then after fetching the results, use the scrape tool to get the content of the relevantpage(s) that you want to scrape\\n\\n**Usage Example without formats (Preferred):**\\n```json\\n{\\n  \"name\": \"firecrawl_search\",\\n  \"arguments\": {\\n    \"query\": \"top AI companies\",\\n    \"limit\": 5,\\n    \"sources\": [\\n      \"web\"\\n    ]\\n  }\\n}\\n```\\n**Usage Example with formats:**\\n```json\\n{\\n  \"name\": \"firecrawl_search\",\\n  \"arguments\": {\\n    \"query\": \"latest AI research papers 2023\",\\n    \"limit\": 5,\\n    \"lang\": \"en\",\\n    \"country\": \"us\",\\n    \"sources\": [\\n      \"web\",\\n      \"images\",\\n      \"news\"\\n    ],\\n    \"scrapeOptions\": {\\n      \"formats\": [\"markdown\"],\\n      \"onlyMainContent\": true\\n    }\\n  }\\n}\\n```\\n**Returns:** Array of search results (with optional scraped content).\\n',\n",
       " 'inputSchema': {'$schema': 'https://json-schema.org/draft/2020-12/schema',\n",
       "  'type': 'object',\n",
       "  'properties': {'query': {'type': 'string', 'minLength': 1},\n",
       "   'limit': {'type': 'number'},\n",
       "   'tbs': {'type': 'string'},\n",
       "   'filter': {'type': 'string'},\n",
       "   'location': {'type': 'string'},\n",
       "   'sources': {'type': 'array',\n",
       "    'items': {'type': 'object',\n",
       "     'properties': {'type': {'type': 'string',\n",
       "       'enum': ['web', 'images', 'news']}},\n",
       "     'required': ['type'],\n",
       "     'additionalProperties': False}},\n",
       "   'scrapeOptions': {'type': 'object',\n",
       "    'properties': {'formats': {'type': 'array',\n",
       "      'items': {'anyOf': [{'type': 'string',\n",
       "         'enum': ['markdown',\n",
       "          'html',\n",
       "          'rawHtml',\n",
       "          'screenshot',\n",
       "          'links',\n",
       "          'summary',\n",
       "          'changeTracking']},\n",
       "        {'type': 'object',\n",
       "         'properties': {'type': {'type': 'string', 'const': 'json'},\n",
       "          'prompt': {'type': 'string'},\n",
       "          'schema': {'type': 'object',\n",
       "           'propertyNames': {'type': 'string'},\n",
       "           'additionalProperties': {}}},\n",
       "         'required': ['type'],\n",
       "         'additionalProperties': False},\n",
       "        {'type': 'object',\n",
       "         'properties': {'type': {'type': 'string', 'const': 'screenshot'},\n",
       "          'fullPage': {'type': 'boolean'},\n",
       "          'quality': {'type': 'number'},\n",
       "          'viewport': {'type': 'object',\n",
       "           'properties': {'width': {'type': 'number'},\n",
       "            'height': {'type': 'number'}},\n",
       "           'required': ['width', 'height'],\n",
       "           'additionalProperties': False}},\n",
       "         'required': ['type'],\n",
       "         'additionalProperties': False}]}},\n",
       "     'parsers': {'type': 'array',\n",
       "      'items': {'anyOf': [{'type': 'string', 'enum': ['pdf']},\n",
       "        {'type': 'object',\n",
       "         'properties': {'type': {'type': 'string', 'enum': ['pdf']},\n",
       "          'maxPages': {'type': 'integer', 'minimum': 1, 'maximum': 10000}},\n",
       "         'required': ['type'],\n",
       "         'additionalProperties': False}]}},\n",
       "     'onlyMainContent': {'type': 'boolean'},\n",
       "     'includeTags': {'type': 'array', 'items': {'type': 'string'}},\n",
       "     'excludeTags': {'type': 'array', 'items': {'type': 'string'}},\n",
       "     'waitFor': {'type': 'number'},\n",
       "     'mobile': {'type': 'boolean'},\n",
       "     'skipTlsVerification': {'type': 'boolean'},\n",
       "     'removeBase64Images': {'type': 'boolean'},\n",
       "     'location': {'type': 'object',\n",
       "      'properties': {'country': {'type': 'string'},\n",
       "       'languages': {'type': 'array', 'items': {'type': 'string'}}},\n",
       "      'additionalProperties': False},\n",
       "     'storeInCache': {'type': 'boolean'},\n",
       "     'maxAge': {'type': 'number'}},\n",
       "    'additionalProperties': False}},\n",
       "  'required': ['query'],\n",
       "  'additionalProperties': False},\n",
       " 'required': ['query'],\n",
       " 'optional': ['limit',\n",
       "  'tbs',\n",
       "  'filter',\n",
       "  'location',\n",
       "  'sources',\n",
       "  'scrapeOptions']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functional_mcp import load_server\n",
    "\n",
    "server = load_server(\"http://localhost:4000/mcp\", headers={\"Authorization\": \"Bearer sk-1234\"})\n",
    "print(server)\n",
    "tools = server.tools\n",
    "print(tools)\n",
    "\n",
    "tools.firecrawl_search.schema.toDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34330a42",
   "metadata": {},
   "source": [
    "### STDIO Config\n",
    "Standard input/output servers require their config, and in order to maintain composability here, the way that we handle this is to maintain our DSL structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae83149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "from functional_mcp import load_config, create_config\n",
    "from fastmcp.client.transports import (\n",
    "    NpxStdioTransport,\n",
    "    StdioTransport,\n",
    "    StreamableHttpTransport,\n",
    "    PythonStdioTransport,\n",
    "    NodeStdioTransport,\n",
    "    UvStdioTransport,\n",
    "    NodeStdioTransport\n",
    ")\n",
    "\n",
    "\n",
    "transport_type = Literal[\"stdio\", \"http\", \"python\", \"node\", \"uv\", \"npx\"]\n",
    "\n",
    "#fluent build of the config, properly mapped for the transfort\n",
    "stdio_config = create_config(\"my-stido-config\")\n",
    "\n",
    "stdio_config.add_env(\"foo\").with_value(\"bar\")\n",
    "\n",
    "stdio_config.add_env(\"baz\").with_value(\"qux\")\n",
    "\n",
    "stdio_config.add_env(\"quux\").with_value(\"corge\")\n",
    "\n",
    "alternative_config = create_config(\"alternative-config\").with_args(\n",
    "    {\n",
    "        \"env\": {\n",
    "            \"foo\": \"bar\",\n",
    "            \"baz\": \"qux\",\n",
    "            \"quux\": \"corge\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "studio_config.init() # compile and valdiate the config. we could even cloudpickle it.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
